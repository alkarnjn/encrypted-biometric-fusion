{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Luke Sperling\n",
    "Created: 04-04-22\n",
    "Modified: 07-08-22\n",
    "Training procedure for FHE-aware Learning (HEFT). Also includes version where exact normalization is used. This is considered not FHE-aware Learning.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from model import Linear_Feature_Fusion, Linear_Feature_Fusion_FHEaware\n",
    "# from data_generation import data_gen\n",
    "\n",
    "from ROC import New_ROC_AUC\n",
    "\n",
    "\n",
    "#polynomial approx of norm\n",
    "def approximate_inv_norm(x_in):\n",
    "    coeffs = [[0.42084296,-1.81897596,2.51308415]]\n",
    "    x = torch.linalg.norm(x_in)**2\n",
    "    result = 0\n",
    "    for coeff_list in coeffs:\n",
    "        result = coeff_list[0]\n",
    "        for i in range(1,len(coeff_list)):\n",
    "            result = result * x + coeff_list[i]\n",
    "        x = result\n",
    "        result = 0\n",
    "    return x\n",
    "\n",
    "def train_exact(gamma,iters,spec_margin=None,spec_lamb=None):\n",
    "    \"\"\"\n",
    "    train using the exact inverse norm instead of approx inverse norm\n",
    "    gamma - output dimension\n",
    "    iters - number of epochs to train\n",
    "    spec_margin, spec_lamb - margin and lambda hyperparameters may be specified\n",
    "    \"\"\"\n",
    "    random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    \n",
    "    #start by loading feature vectors and labels\n",
    "    a = []\n",
    "    A_infile = open(\"feature-extraction/extractions/VGGFace_vgg_cplfw.txt\",'r')\n",
    "    for line in A_infile:\n",
    "        line = line.strip().split()\n",
    "        a.append(torch.tensor([float(char) for char in line]))\n",
    "\n",
    "    L = []\n",
    "    L_infile = open(\"feature-extraction/extractions/deep_speaker_librispeech_google_labels.txt\",'r')\n",
    "    \n",
    "    l_dict = {}\n",
    "    for line in L_infile:\n",
    "        line = line.strip()\n",
    "        if line not in l_dict:\n",
    "            l_dict[line] = 0\n",
    "        l_dict[line] += 1\n",
    "        L.append(line.strip())\n",
    "    #convert from unique string labels to integers\n",
    "    #from https://stackoverflow.com/questions/43203215/map-unique-strings-to-integers-in-python\n",
    "    d = dict([(y,x+1) for x,y in enumerate(sorted(set(L)))])\n",
    "    L = [d[x] for x in L]\n",
    "    \n",
    "    l_dict = {}\n",
    "    for l in L:\n",
    "        if l not in l_dict:\n",
    "            l_dict[l] = 0\n",
    "        l_dict[l] += 1\n",
    "    \n",
    "    b = []\n",
    "    B_infile = open(\"feature-extraction/extractions/deep_speaker_librispeech_google.txt\",'r')\n",
    "    for line in B_infile:\n",
    "        line = line.strip().split()\n",
    "        b.append(torch.tensor([float(char) for char in line]))\n",
    "    \n",
    "    b, L = (list(t) for t in zip(*sorted(zip(b, L),key=itemgetter(1))))\n",
    "    \n",
    "    \n",
    "    #pair all possible combinations of feature vectors\n",
    "    completed = 0\n",
    "    a_index = 0\n",
    "    a_sub_index = 0\n",
    "    b_index = 0\n",
    "    samples_per_face = 2\n",
    "    samples_per_voice = 10\n",
    "    num_each_class = samples_per_face * samples_per_voice\n",
    "    true_a = []\n",
    "    true_b = []\n",
    "    true_L = []\n",
    "    \n",
    "    \n",
    "    num_classes = 188\n",
    "    split1 = 0\n",
    "    split2 = 0\n",
    "    \n",
    "    last_l = L[0]\n",
    "    i = 0\n",
    "    while completed <= 188:\n",
    "        #for i, label in enumerate(L):\n",
    "        label = L[b_index]\n",
    "        for j in range(2):\n",
    "            a_sub_index = j\n",
    "            print(a_index+a_sub_index, b_index, label)\n",
    "            true_a.append(a[a_index+a_sub_index])\n",
    "            true_b.append(b[b_index])\n",
    "            true_L.append(L[b_index])\n",
    "        b_index += 1\n",
    "        \n",
    "        if b_index >= len(L):\n",
    "            break\n",
    "        label = L[b_index]\n",
    "        if label != last_l:\n",
    "            last_l = label\n",
    "            a_index += 2\n",
    "            completed += 1\n",
    "            if split1 == 0 and a_index/2 >= math.floor(num_classes * 0.2):\n",
    "                split1 = i\n",
    "            if split2 == 0 and a_index/2 >= math.floor(num_classes * 0.4):\n",
    "                split2 = i\n",
    "        i += 1\n",
    "\n",
    "    print(len(true_a), len(true_b))\n",
    "    \n",
    "    print(\"Splits:\",split1,split2)\n",
    "    \n",
    "    A = torch.stack(true_a)\n",
    "    print(\"A:\",A.shape)\n",
    "    B = torch.stack(true_b)\n",
    "    print(\"B:\",B.shape)\n",
    "    \n",
    "    \n",
    "    L2 = L\n",
    "    L = torch.tensor(true_L)\n",
    "    print(\"L:\",L.shape)\n",
    "    print(L)\n",
    "    \n",
    "    \n",
    "    #now we can create our final dataset\n",
    "    X = torch.cat((A,B),dim=1)\n",
    "    X_train = X[split2:,:]\n",
    "    X_test = X[split1:split2,:]\n",
    "    X_val = X[:split1,:]\n",
    "    \n",
    "    L_train = L[split2:]\n",
    "    L_test = L[split1:split2]\n",
    "    L_val = L[:split1]\n",
    "    \n",
    "    #randomize train set order to aid in batches\n",
    "    temp = list(zip(X_train.tolist(), L_train))\n",
    "    random.shuffle(temp)\n",
    "    res1, res2 = zip(*temp)\n",
    "    # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "    X_train_list, L_train_list = list(res1), list(res2)\n",
    "    \n",
    "    X_train = torch.tensor(X_train_list)\n",
    "    L_train = torch.tensor(L_train_list)\n",
    "    \n",
    "    print(\"X_train shuffled:\",X_train.shape)\n",
    "    print(\"L_train shuffled:\",L_train.shape)\n",
    "    \n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    if not os.path.exists(\"data/dataset\"):\n",
    "        os.mkdir(\"data/dataset\")\n",
    "    if not os.path.exists(\"data/exact_results\"):\n",
    "        os.mkdir(\"data/exact_results\")\n",
    "    if not os.path.exists(\"data/exact_results/\"):\n",
    "        os.mkdir(\"data/dataset\")\n",
    "\n",
    "    outfile_a = open(\"data/dataset/A_values.txt\",'w')\n",
    "    for row in A.tolist():\n",
    "        for item in row:\n",
    "            outfile_a.write(str(f'{item:.9f}'))\n",
    "            outfile_a.write(\" \")\n",
    "        outfile_a.write(\"\\n\")\n",
    "    outfile_a.close()\n",
    "    \n",
    "    outfile_b = open(\"data/dataset/B_values.txt\",'w')\n",
    "    for row in B.tolist():\n",
    "        for item in row:\n",
    "            outfile_b.write(str(f'{item:.9f}'))\n",
    "            outfile_b.write(\" \")\n",
    "        outfile_b.write(\"\\n\")\n",
    "    outfile_b.close()\n",
    "    \n",
    "    outfile_a_train = open(\"data/dataset/A_values_train.txt\",'w')\n",
    "    for row in A.tolist()[split2:]:\n",
    "        for item in row:\n",
    "            outfile_a_train.write(str(f'{item:.9f}'))\n",
    "            outfile_a_train.write(\" \")\n",
    "        outfile_a_train.write(\"\\n\")\n",
    "    outfile_a_train.close()\n",
    "    \n",
    "    outfile_b_train = open(\"data/dataset/B_values_train.txt\",'w')\n",
    "    for row in B.tolist()[split2:]:\n",
    "        for item in row:\n",
    "            outfile_b_train.write(str(f'{item:.9f}'))\n",
    "            outfile_b_train.write(\" \")\n",
    "        outfile_b_train.write(\"\\n\")\n",
    "    outfile_b_train.close()\n",
    "\n",
    "    outfile_a_test = open(\"data/dataset/A_values_test.txt\",'w')\n",
    "    for row in A.tolist()[split1:split2]:\n",
    "        for item in row:\n",
    "            outfile_a_test.write(str(f'{item:.9f}'))\n",
    "            outfile_a_test.write(\" \")\n",
    "        outfile_a_test.write(\"\\n\")\n",
    "    outfile_a_test.close()\n",
    "\n",
    "    \n",
    "    outfile_b_test = open(\"data/dataset/B_values_test.txt\",'w')\n",
    "    for row in B.tolist()[split1:split2]:\n",
    "        for item in row:\n",
    "            outfile_b_test.write(str(f'{item:.9f}'))\n",
    "            outfile_b_test.write(\" \")\n",
    "        outfile_b_test.write(\"\\n\")\n",
    "    outfile_b_test.close()\n",
    "    \n",
    "    outfile_a_test = open(\"data/dataset/A_values_test_transpose.txt\",'w')\n",
    "    for row in A[split1:split2,:].T.tolist():\n",
    "        for item in row:\n",
    "            outfile_a_test.write(str(f'{item:.9f}'))\n",
    "            outfile_a_test.write(\" \")\n",
    "        outfile_a_test.write(\"\\n\")\n",
    "    outfile_a_test.close()\n",
    "    \n",
    "    outfile_b_test = open(\"data/dataset/B_values_test_transpose.txt\",'w')\n",
    "    for row in B[split1:split2,:].T.tolist():\n",
    "        for item in row:\n",
    "            outfile_b_test.write(str(f'{item:.9f}'))\n",
    "            outfile_b_test.write(\" \")\n",
    "        outfile_b_test.write(\"\\n\")\n",
    "    outfile_b_test.close()\n",
    "    \n",
    "    outfile_a_val = open(\"data/dataset/A_values_val.txt\",'w')\n",
    "    for row in A.tolist()[:split1]:\n",
    "        outfile_a_val.write(\"[\")\n",
    "        for item in row:\n",
    "            outfile_a_val.write(str(f'{item:.9f}'))\n",
    "            outfile_a_val.write(\" \")\n",
    "        outfile_a_val.write(\"]\")\n",
    "        outfile_a_val.write(\"\\n\")\n",
    "    outfile_a_val.close()\n",
    "    \n",
    "    outfile_b_val = open(\"data/dataset/B_values_val.txt\",'w')\n",
    "    for row in B.tolist()[:split1]:\n",
    "        outfile_b_val.write(\"[\")\n",
    "        for item in row:\n",
    "            outfile_b_val.write(str(f'{item:.9f}'))\n",
    "            outfile_b_val.write(\" \")\n",
    "        outfile_b_val.write(\"]\")\n",
    "        outfile_b_val.write(\"\\n\")\n",
    "    outfile_b_val.close()\n",
    "    \n",
    "    outfile_x_test = open(\"data/dataset/X_values_test.txt\",'w')\n",
    "    for row in X.tolist()[split1:split2]:\n",
    "        outfile_x_test.write(\"[\")\n",
    "        for item in row:\n",
    "            outfile_x_test.write(str(f'{item:.9f}'))\n",
    "            outfile_x_test.write(\" \")\n",
    "        outfile_x_test.write(\"]\")\n",
    "        outfile_x_test.write(\"\\n\")\n",
    "    outfile_x_test.close()\n",
    "    \n",
    "    outfile_L = open(\"data/dataset/L_values_val.txt\",'w')\n",
    "    outfile_L.write(str(L_val.tolist()))\n",
    "    outfile_L.close()\n",
    "    \n",
    "    outfile_L = open(\"data/dataset/L_values_test.txt\",'w')\n",
    "    outfile_L.write(str(L_test.tolist()))\n",
    "    outfile_L.close()\n",
    "    \n",
    "    A2 = torch.stack(a)\n",
    "    B2 = torch.stack(b)\n",
    "    L2 = torch.tensor(L2)\n",
    "    \n",
    "    outfile_b_test = open(\"data/dataset/A_values_test_unique.txt\",'w')\n",
    "    for row in A2.tolist()[math.floor(0.2*num_classes)*2:math.floor(0.4*num_classes)*2]:\n",
    "        for item in row:\n",
    "            outfile_b_test.write(str(f'{item:.9f}'))\n",
    "            outfile_b_test.write(\" \")\n",
    "        outfile_b_test.write(\"\\n\")\n",
    "    outfile_b_test.close()\n",
    "    \n",
    "    outfile_b_test = open(\"data/dataset/B_values_test_unique.txt\",'w')\n",
    "    for row in B2.tolist()[split1//2:split2//2]:\n",
    "        for item in row:\n",
    "            outfile_b_test.write(str(f'{item:.9f}'))\n",
    "            outfile_b_test.write(\" \")\n",
    "        outfile_b_test.write(\"\\n\")\n",
    "    outfile_b_test.close()\n",
    "    \n",
    "    outfile_L = open(\"data/dataset/L_values_test_unique.txt\",'w')\n",
    "    outfile_L.write(str(L2[split1//2:split2//2].tolist()))\n",
    "    outfile_L.close()\n",
    "    \n",
    "    #Hyperparameters\n",
    "    lambs = [0.01,0.1,0.25,0.5,0.75,0.99]\n",
    "    margins = [0.1,0.25,0.5,0.75,1.0]\n",
    "    iterations = iters\n",
    "\n",
    "    regularizers = [0]\n",
    "    \n",
    "    rate = 0.005\n",
    "    decay = 0.0001\n",
    "    \n",
    "    #function parameters\n",
    "    if spec_margin:\n",
    "        margins = [spec_margin]\n",
    "    if spec_lamb:\n",
    "        lambs = [spec_lamb]\n",
    "    \n",
    "    \n",
    "    aucs = []\n",
    "    #repeat training for each hyperparam combination\n",
    "    for reg in regularizers:\n",
    "        for margin in margins:\n",
    "            for lamb in lambs:\n",
    "                print(\"Lambda value of\", lamb, \"Margin value of\", margin, \"regularizer of\",reg)\n",
    "                M = []\n",
    "                V = []\n",
    "                randie = 123\n",
    "                print(\"seed of:\",randie)\n",
    "                model = Linear_Feature_Fusion(X_train,M,V,L_train,gamma,margin,lamb,regularization=reg,seed=randie)\n",
    "                best_loss = model.loss()\n",
    "                best_P = model.P\n",
    "                print(\"Initial loss:\",best_loss)\n",
    "                losses = []\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=rate, weight_decay=decay)\n",
    "                for i in range(iterations):\n",
    "                    loss = model.loss()\n",
    "                    if loss <= best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_P = model.P\n",
    "                    losses.append(loss)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if i%10 == 0:\n",
    "                        print(\"Iteration\",str(i) + \"/\" + str(iterations))\n",
    "                        print(model.loss())\n",
    "                X_prime = torch.mm(X_val,best_P)\n",
    "                print(X_prime.shape)\n",
    "\n",
    "                for i in range(X_prime.shape[0]):\n",
    "                    X_prime[i,:]=torch.mul(X_prime[i,:], approximate_inv_norm(X_prime[i,:]))\n",
    "                X_prime = torch.mm(X_val,best_P)\n",
    "                print(X_prime.shape)\n",
    "\n",
    "                for i in range(X_prime.shape[0]):\n",
    "                    X_prime[i,:]=torch.div(X_prime[i,:], torch.linalg.norm(X_prime[i,:]))\n",
    "                \n",
    "                auc = New_ROC_AUC(X_prime, L_val)\n",
    "                aucs.append((margin,lamb,auc))\n",
    "                print(\"AUC of lambda=\"+str(lamb)+\", margin=\"+str(margin)+ \"_reg=\" + str(reg) + \":\", auc)\n",
    "\n",
    "                p_best_file_name = \"data/exact_results/exact_best_P_value_transpose_lambda=\" + str(lamb) + \"_margin=\" + str(margin) + \"_gamma=\" + str(gamma)  + \"_reg=\" + str(reg) + \".txt\"\n",
    "                outfile_p_t = open(p_best_file_name,'w')\n",
    "                P_final_t = best_P.T\n",
    "                P_final_t = str(P_final_t.tolist())\n",
    "                outfile_p_t.write(P_final_t)\n",
    "                outfile_p_t.close()\n",
    "                \n",
    "    print(\"(margin, lambda, Validation AUC)\")\n",
    "    print(aucs)\n",
    "\n",
    "def train(gamma,iters,spec_margin=None,spec_lamb=None):\n",
    "    \"\"\"\n",
    "    train using the approx inverse norm. same as train_exact otherwise\n",
    "    gamma - output dimension\n",
    "    iters - number of epochs to train\n",
    "    spec_margin, spec_lamb - margin and lambda hyperparameters may be specified\n",
    "    \"\"\"\n",
    "    random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    #start by loading feature vectors and labels\n",
    "    a = []\n",
    "    A_infile = open(\"feature-extraction/extractions/VGGFace_vgg_cplfw.txt\",'r')\n",
    "    for line in A_infile:\n",
    "        line = line.strip().split()\n",
    "        a.append(torch.tensor([float(char) for char in line]))\n",
    "    \n",
    "    L = []\n",
    "    L_infile = open(\"feature-extraction/extractions/deep_speaker_librispeech_google_labels.txt\",'r')\n",
    "    \n",
    "    l_dict = {}\n",
    "    for line in L_infile:\n",
    "        line = line.strip()\n",
    "        if line not in l_dict:\n",
    "            l_dict[line] = 0\n",
    "        l_dict[line] += 1\n",
    "        \n",
    "        L.append(line.strip())\n",
    "    \n",
    "    #convert from unique string labels to integers\n",
    "    #from https://stackoverflow.com/questions/43203215/map-unique-strings-to-integers-in-python\n",
    "    d = dict([(y,x+1) for x,y in enumerate(sorted(set(L)))])\n",
    "    L = [d[x] for x in L]\n",
    "    \n",
    "    l_dict = {}\n",
    "    for l in L:\n",
    "        if l not in l_dict:\n",
    "            l_dict[l] = 0\n",
    "        l_dict[l] += 1\n",
    "    #print(L)\n",
    "    \n",
    "    b = []\n",
    "    B_infile = open(\"feature-extraction/extractions/deep_speaker_librispeech_google.txt\",'r')\n",
    "    for line in B_infile:\n",
    "        line = line.strip().split()\n",
    "        b.append(torch.tensor([float(char) for char in line]))\n",
    "    \n",
    "    \n",
    "    b, L = (list(t) for t in zip(*sorted(zip(b, L),key=itemgetter(1))))\n",
    "    \n",
    "    #pair all possible combinations of feature vectors\n",
    "    completed = 0\n",
    "    a_index = 0\n",
    "    a_sub_index = 0\n",
    "    b_index = 0\n",
    "    samples_per_face = 2\n",
    "    samples_per_voice = 10\n",
    "    num_each_class = samples_per_face * samples_per_voice\n",
    "    true_a = []\n",
    "    true_b = []\n",
    "    true_L = []\n",
    "    \n",
    "    \n",
    "    num_classes = 188\n",
    "    split1 = 0\n",
    "    split2 = 0\n",
    "    \n",
    "    last_l = L[0]\n",
    "    i = 0\n",
    "    while completed <= 188:\n",
    "        label = L[b_index]\n",
    "        for j in range(2):\n",
    "            a_sub_index = j\n",
    "            print(a_index+a_sub_index, b_index, label)\n",
    "            true_a.append(a[a_index+a_sub_index])\n",
    "            true_b.append(b[b_index])\n",
    "            true_L.append(L[b_index])\n",
    "        b_index += 1\n",
    "        \n",
    "        if b_index >= len(L):\n",
    "            break\n",
    "        label = L[b_index]\n",
    "        if label != last_l:\n",
    "            last_l = label\n",
    "            a_index += 2\n",
    "            completed += 1\n",
    "            if split1 == 0 and a_index/2 >= math.floor(num_classes * 0.2):\n",
    "                split1 = i\n",
    "            if split2 == 0 and a_index/2 >= math.floor(num_classes * 0.4):\n",
    "                split2 = i\n",
    "        i += 1\n",
    "        \n",
    "    print(len(true_a), len(true_b))\n",
    "    \n",
    "    \n",
    "    print(\"Splits:\",split1,split2)\n",
    "    \n",
    "    A = torch.stack(true_a)\n",
    "    print(\"A:\",A.shape)\n",
    "    B = torch.stack(true_b)\n",
    "    print(\"B:\",B.shape)\n",
    "    \n",
    "    L = torch.tensor(true_L)\n",
    "    print(\"L:\",L.shape)\n",
    "    print(L)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = torch.cat((A,B),dim=1)\n",
    "    X_train = X[split2:,:]\n",
    "    X_test = X[split1:split2,:]\n",
    "    X_val = X[:split1,:]\n",
    "    \n",
    "    L_train = L[split2:]\n",
    "    L_test = L[split1:split2]\n",
    "    L_val = L[:split1]\n",
    "    \n",
    "    #randomize train set order to aid in batches\n",
    "    temp = list(zip(X_train.tolist(), L_train))\n",
    "    random.shuffle(temp)\n",
    "    res1, res2 = zip(*temp)\n",
    "    # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "    X_train_list, L_train_list = list(res1), list(res2)\n",
    "    \n",
    "    X_train = torch.tensor(X_train_list)\n",
    "    L_train = torch.tensor(L_train_list)\n",
    "    \n",
    "    print(\"X_train shuffled:\",X_train.shape)\n",
    "    print(\"L_train shuffled:\",L_train.shape)\n",
    "    \n",
    "    if not os.path.exists(\"data/degree=3strict\"):\n",
    "        os.mkdir(\"data/degree=3strict\")\n",
    "    if not os.path.exists(\"data/degree=2strict\"):\n",
    "        os.mkdir(\"data/degree=2strict\")\n",
    "    \n",
    "    \n",
    "    #Hyperparameters\n",
    "    lambs = [0.01,0.1,0.25,0.5,0.75,0.99]\n",
    "    margins = [0.1,0.25,0.5,0.75,1.0]\n",
    "    iterations = iters\n",
    "\n",
    "    regularizers = [0]\n",
    "\n",
    "    rate = 0.005\n",
    "    decay = 0.0001\n",
    "\n",
    "    if spec_margin:\n",
    "        margins = [spec_margin]\n",
    "    if spec_lamb:\n",
    "        lambs = [spec_lamb]\n",
    "\n",
    "    #run full training for every hyperparam combination\n",
    "    aucs = []\n",
    "    for reg in regularizers:\n",
    "        for margin in margins:\n",
    "            for lamb in lambs:\n",
    "                print(\"Lambda value of\", lamb, \"Margin value of\", margin, \"regularizer of\",reg)\n",
    "                \n",
    "                M = []\n",
    "                V = []\n",
    "                randie = 123\n",
    "                print(\"seed of:\",randie)\n",
    "                # print(\"rate, anneal_rate:\", rate, anneal_rate)\n",
    "                model = Linear_Feature_Fusion_FHEaware(X_train,M,V,L_train,gamma,margin,lamb,regularization=reg,seed=randie)\n",
    "                \n",
    "                #first we select a scale such that our polynomial function will work properly\n",
    "                total = 0.0\n",
    "                P_temp = torch.div(model.P,torch.linalg.norm(model.P))\n",
    "                min_norm = 10000\n",
    "                for c in range(X_train.shape[0]):\n",
    "                    norm = torch.linalg.norm(torch.matmul(P_temp.T,X_train[c,:].T))\n",
    "                    total += norm\n",
    "                total = float(total)\n",
    "                avg = total / X_train.shape[0]\n",
    "                model.scale = 1.525**0.5 / avg\n",
    "                \n",
    "                best_loss = model.loss()\n",
    "                best_P = model.P\n",
    "                best_scale = model.scale\n",
    "                print(\"Initial loss:\",best_loss)\n",
    "                losses = []\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=rate, weight_decay=decay)\n",
    "                for i in range(iterations):\n",
    "                    #first we select a scale such that our polynomial function will work\n",
    "                    total = 0.0\n",
    "                    P_temp = torch.div(model.P,torch.linalg.norm(model.P))\n",
    "                    min_norm = 10000\n",
    "                    for c in range(X_train.shape[0]):\n",
    "                        norm = torch.linalg.norm(torch.matmul(P_temp.T,X_train[c,:].T))\n",
    "                        total += norm\n",
    "                        if norm < min_norm:\n",
    "                            min_norm = norm\n",
    "                    min_norm = float(min_norm)\n",
    "                    total = float(total)\n",
    "                    avg = total / X_train.shape[0]\n",
    "                    model.scale = 1.525**0.5 / avg\n",
    "                    \n",
    "                    loss = model.loss()\n",
    "                    if loss <= best_loss:\n",
    "                        best_loss = loss\n",
    "                        best_P = model.P\n",
    "                        best_scale = model.scale\n",
    "                    losses.append(loss)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if i%10 == 0:\n",
    "                        print(\"Iteration\",str(i) + \"/\" + str(iterations))\n",
    "                        print(loss)\n",
    "                best_P = torch.div(best_P,torch.linalg.norm(best_P))\n",
    "                \n",
    "                print(\"percentage of escape:\",model.escape/model.tote)\n",
    "                \n",
    "                #calculate scale for best P\n",
    "                total = 0.0\n",
    "                min_norm = 10000\n",
    "                for c in range(X_train.shape[0]):\n",
    "                    norm = torch.linalg.norm(torch.matmul(P_temp.T,X_train[c,:].T))\n",
    "                    total += norm\n",
    "                total = float(total)\n",
    "                avg = total / X_train.shape[0]\n",
    "                best_scale = 1.525**0.5 / avg\n",
    "                \n",
    "                best_P = torch.mul(best_P, best_scale)\n",
    "                \n",
    "                X_prime = torch.mm(X_val,best_P)\n",
    "                print(X_prime.shape)\n",
    "                for i in range(X_prime.shape[0]):\n",
    "                    X_prime[i,:]=torch.div(X_prime[i,:], torch.linalg.norm(X_prime[i,:]))\n",
    "                auc = New_ROC_AUC(X_prime, L_val)\n",
    "                print(\"AUC (using exact normalziation) of lambda=\"+str(lamb)+\", margin=\"+str(margin)+ \"_reg=\" + str(reg) + \":\", auc)\n",
    "                \n",
    "                X_prime = torch.mm(X_val,best_P)\n",
    "                print(X_prime.shape)\n",
    "                for i in range(X_prime.shape[0]):\n",
    "                    X_prime[i,:]=torch.mul(X_prime[i,:], approximate_inv_norm(X_prime[i,:]))\n",
    "                \n",
    "                auc = New_ROC_AUC(X_prime, L_val)\n",
    "                aucs.append((margin,lamb,auc,best_scale))\n",
    "                print(\"AUC of lambda=\"+str(lamb)+\", margin=\"+str(margin)+ \"_reg=\" + str(reg) + \":\", auc)\n",
    "                \n",
    "                p_best_file_name = \"data/degree=2strict/approximate_best_P_value_transpose_lambda=\" + str(lamb) + \"_margin=\" + str(margin) + \"_gamma=\" + str(gamma)  + \"_reg=\" + str(reg) + \".txt\"\n",
    "                outfile_p_t = open(p_best_file_name,'w')\n",
    "                P_final_t = best_P.T\n",
    "                P_final_t = str(P_final_t.tolist())\n",
    "                outfile_p_t.write(P_final_t)\n",
    "                outfile_p_t.close()\n",
    "                \n",
    "    print(\"(margin, lambda, Validation AUC), Scale\")\n",
    "    print(aucs)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train_exact(32,1000)\n",
    "    train(32,1000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
